# 요약

## 웹 크롤러

- 검색엔진에 웹페이지의 정보를 수집하기 위한 용도
- 주 사용처
  - 검색 엔진 인덱싱: 검색 엔진에서 사용할 정보를 인덱싱
  - 웹 아카이빙: 웹의 페이지를 저장
  - 웹 마이닝: 검색한 정보들 중에 유용한 정보를 추출
  - 웹 모니터링: 저작권 위반한 사례를 검색

## 웹 크롤링의 기본 로직

1. 시작 URL을 접속하여 URL내에 속한 모든 URL들을 수집
2. URL페이지를 다운로드
3. 다운로드한 페이지에서 URL을 재추출

## 개략적인 구성 요소

- 시작 URL: 크롤링을 시작하는 지점(웹페이지의 루트페이지)
- 미수집 URL 저장소: 아직 수집을 하지 않은 URL, Queue구조로 구성
- HTML Downlaoder: URL에 접속해 웹페이지를 다운로드
  - robots.txt규칙을 준수
- 도메인 이름 변환기: 도메인 이름을 ip로 변경해주는 역할
  - 도메인에 매핑되는 ip를 캐싱하여 도메인을 ip로 바꾸는 과정을 최적화가 가능
- 콘텐츠 파서: 다운로드한 페이지에서 필요한 정보를 파싱 및 검증
- 콘텐츠 저장소: downloader가 받은 웹 페이지를 저장
  - 유효기간, 데이터 크기, 접근 빈도, 데이터 유형등을 고려가 필요
- URL 추출기: html페이지에 적힌 상대경로를 절대경로로 변경
- URL 필터: 이미지, 파일 위치등 html이 아닌 url들을 필터링
- URL 저장소: 이미 방문한 url 저장소
  
### 고려사항1: 중복된 콘텐츠 확인

- 웹페이지중 29% 가량은 중복된 콘텐츠를 가짐
  - 즉 중보된 콘텐츠를 확보하지 않으면 전체 데이터중 29%는 중복된 콘텐츠일 가능성이 높음
- 콘텐츠 확인
  - 문자열 확인: 문자열 정보가 같으면 중복된 콘텐츠로 확인 => 모든 문자를 확인해야하므로 성능이 낮음
  - 해시: html페이지를 해싱하여 비교

### 고려사항2: 이미 방문한 URL 확인

- 이미 방문해 수집을 완료한 URL을 재방문하지 않도록 저장
- 주로 bloom filter, hash table로 구현

### 구성요소간 작업 흐름

1. 시작 URL에서 미수집 URL들을 추출해 저장소에 저장
2. HTML Downloader는 미수집 URL저장소에서 URL을 파싱해 다운로드
   1. 이 과정에서 도메인 이름 변환기에 도메인을 ip로 변경하는 작업을 제공
3. 콘텐츠 파서에서 필요한 정보를 추출
4. 파싱된 정보를 콘텐츠 저장소에서 조회해 중복 여부를 확인하고 중복된 정보를 버림
5. 파싱된 정보 중  url 관련 정보를 URL 추출기로 추출
6. 추출한 URL중에 URL 필터를 걸러 html 페이지만 필터링
7. 필터링된 URL를 URL저장소에서 검색해서 방문한 URL 여부를 확인
8. 방문하지 않았으면 미수집 URL 저장소에 삽입
9. 2부터 돌아가 반복


## 상세 설계시 고려사항

### DFS vs BFS

- 크롤링을 하는 웹패이지는 페이지를 노드, 페이지 내에 속한 링크를 edge로 이루어진 그래프로 구성
- DFS 보단 BFS를 주로 선택
  - 웹페이지 특성항 depth가 무한대라 될 수 있기 때문
- BFS를 이용한다면 고려할 점
  - 한 페이지에서 나오는 링크의 대부분은 같은 서버
    - 대부분의 링크는 서버의 내부 링크일 가능성이 큼
    - 같은 서버에 계속 크롤링을 요청하면 예의 없는(imposlite) 크롤러로 간주될 수 있음
  - 크롤링의 우선순위 지정:
    - 표준 BFS는 우선순위 없이 주변 노드들부터 탐색
    -  사람들이 자주 다니는 페이지, 업데이트 빈도 등 우선순위를 지정하기 위한 추가적인 구현이 필요

### 미수집 URL 저장소 구현 방법

- 예의 바른 크롤러를 만드는 원칙: 동일한 웹사이트는 한번만 방문
- 이를 위해 이미 방문한 웹사이트는 재방문을 하지 않아야 함

### 같은 페이지는 한번만 접속하게 하는 설계

- 구성요소
  - 매핑 테이블: 호스트이름, 큐와 매핑된 테이블로서 호스트에 따라 어느 큐가 처리할지를 보관하는 테이블
  - 큐 라우터: 매핑테이블에서 조회해서 어느 큐에 크롤링 작업을 할당할지를 지정
    - front: url중에 어느 url을 우선으로 처리할 지를 결정
    - back: 크롤러가 예의바르게 동작함을 보장
  - 큐: 작업들이 저장되어 있는 요소
  - 큐 선택기: 작업이 저장되어 있는 큐를 순회하여 크롤링할 URL을 선택해서 스레드에 분배
  - 작업 스레드: url에 접속해 웹페이지를 다운로드 받는 스레드
- front에서 우선으로 처리할 url을 지정하고 back에서 크롤러가 예의바르게 동작함을 보장하는 설계

### 크롤링 성능 최적화

- 분산 크롤링: 크롤링할 URL들을 여러 서버들에게 분산해 처리하는 정책
- 도메인 이름 캐싱(DNS Resolver): DNS요청을 캐싱하여 크롤링 서버가 DNS요청을 보내지 않도록 방지. 그러나 ip 가 변경될 수 있기에 주기적인 업데이트가 필요함
- 지역성(locality): 크롤링을 하는 서버와 당하는 서버의 물리적인 거리가 가깝게 지정
- 짧은 wait timeout: 크롤링을 위해 접속한 URL이 크롤링 당하는 서버의 문제로 먹통이 될 수 있기에 무한정 기다리기 보다 응답시간을 지정해놓고 시간을 초과하면 이 요청은 버리는 프로세스가 필요

### 안정적인 크롤링을 위한 설계

- 안전 해싱을 이용한 다운로드 서버의 부하
- 크롤링의 상태 수집 및 데이터 저장
- 예의처리
- 데이터 검증

### 문제가 있는 콘텐츠 감지 및 회피

- 의도적으로 크롤링을 저해시키는 유해 콘텐츠를 필터링하는 전략이 필요
- 저해시키는 콘텐츠 종류
  1. 중복된 콘텐츠
  2. spider trap: 크롤러가 무한루프에 빠지도록 의미없는 설계를 반복한 웹페이지(abc.com/a/b/a/b/a/b)
  3. 데이터 노이즈: 스팸, 스크립트 코드 등 수집할 콘텐츠와 아무 관련 없는 콘텐츠

### 이외의 고려사항

- SSR를 이용한 동적 웹페이지 크롤링
- 스팸 방지를 이용한 원치않는 페이지 필터링
- db 다중화 및 안정성 확보
- 수평적 확장 구조
- 데이터 분석 솔루션
